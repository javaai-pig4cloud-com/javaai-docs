---
title: "Multimodality API"
---

> "All things that are naturally connected ought to be taught in combination" - John Amos Comenius, "Orbis Sensualium Pictus", 1658

Humans process knowledge simultaneously across multiple modes of data inputs.
The way we learn, our experiences are all multimodal.
We don't have just vision, just audio and just text.

Contrary to those principles, the Machine Learning was often focused on specialized models tailored to process a single modality.
For instance, we developed audio models for tasks like text-to-speech or speech-to-text, and computer vision models for tasks such as object detection and classification.

However, a new wave of multimodal large language models starts to emerge.
Examples include OpenAI's GPT-4o, Google's Vertex AI Gemini 1.5, Anthropic's Claude3, and open source offerings Llama3.2, LLaVA and BakLLaVA are able to accept multiple inputs, including text images, audio and video and generate text responses by integrating these inputs.

<Note>
The multimodal large language model (LLM) features enable the models to process and generate text in conjunction with other modalities such as images, audio, or video.
</Note>

## Spring AI Multimodality

Multimodality refers to a model's ability to simultaneously understand and process information from various sources, including text, images, audio, and other data formats.

The Spring AI Message API provides all necessary abstractions to support multimodal LLMs.

<Frame caption="Spring AI Message API">
  <img src="/images/spring-ai-message-api.jpg" width="800" />
</Frame>

The UserMessage's `content` field is used primarily for text inputs, while the optional `media` field allows adding one or more additional content of different modalities such as images, audio and video.
The `MimeType` specifies the modality type.
Depending on the used LLMs, the `Media` data field can be either the raw media content as a `Resource` object or a `URI` to the content.

<Note>
The media field is currently applicable only for user input messages (e.g., `UserMessage`). It does not hold significance for system messages. The `AssistantMessage`, which includes the LLM response, provides text content only. To generate non-text media outputs, you should utilize one of the dedicated, single-modality models.
</Note>

For example, we can take the following picture (`multimodal.test.png`) as an input and ask the LLM to explain what it sees.

<Frame caption="Multimodal Test Image">
  <img src="/images/multimodal.test.png" width="200" height="200" />
</Frame>

For most of the multimodal LLMs, the Spring AI code would look something like this:

```java
var imageResource = new ClassPathResource("/multimodal.test.png");

var userMessage = new UserMessage(
	"Explain what do you see in this picture?", // content
	new Media(MimeTypeUtils.IMAGE_PNG, this.imageResource)); // media

ChatResponse response = chatModel.call(new Prompt(this.userMessage));
```

or with the fluent [ChatClient](/spring4ai/api/chatclient) API:

```java
String response = ChatClient.create(chatModel).prompt()
		.user(u -> u.text("Explain what do you see on this picture?")
				    .media(MimeTypeUtils.IMAGE_PNG, new ClassPathResource("/multimodal.test.png")))
		.call()
		.content();
```

and produce a response like:

> This is an image of a fruit bowl with a simple design. The bowl is made of metal with curved wire edges that create an open structure, allowing the fruit to be visible from all angles. Inside the bowl, there are two yellow bananas resting on top of what appears to be a red apple. The bananas are slightly overripe, as indicated by the brown spots on their peels. The bowl has a metal ring at the top, likely to serve as a handle for carrying. The bowl is placed on a flat surface with a neutral-colored background that provides a clear view of the fruit inside.

## Supported Multimodal Models

Spring AI provides multimodal support for the following chat models:

<CardGroup cols={2}>
  <Card title="Anthropic Claude 3" icon="cat" href="/spring4ai/api/chat/anthropic-chat#multimodal">
    Supports image inputs for analysis and reasoning
  </Card>
  <Card title="AWS Bedrock Converse" icon="aws" href="/spring4ai/api/chat/bedrock-converse#multimodal">
    Amazon's multimodal conversational AI service
  </Card>
  <Card title="Azure OpenAI" icon="microsoft" href="/spring4ai/api/chat/azure-openai-chat#multimodal">
    Support for GPT-4o and other multimodal models
  </Card>
  <Card title="Mistral AI" icon="cloud" href="/spring4ai/api/chat/mistralai-chat#multimodal">
    Mistral Pixtral models with image understanding capabilities
  </Card>
  <Card title="Ollama" icon="cube" href="/spring4ai/api/chat/ollama-chat#multimodal">
    Support for open-source multimodal models like LLaVA, BakLLaVA, and Llama3.2
  </Card>
  <Card title="OpenAI" icon="openai" href="/spring4ai/api/chat/openai-chat#multimodal">
    GPT-4 and GPT-4o models with vision capabilities
  </Card>
  <Card title="Vertex AI Gemini" icon="google" href="/spring4ai/api/chat/vertexai-gemini-chat#multimodal">
    Google's Gemini models (gemini-1.5-pro-001, gemini-1.5-flash-001) with multimodal capabilities
  </Card>
</CardGroup> 